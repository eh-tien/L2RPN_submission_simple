{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3dc1c0",
   "metadata": {},
   "source": [
    "# This notebook will install necessary dependancies, create and test an agent and save the submission file into the directory containing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1472c",
   "metadata": {},
   "source": [
    "## Uncomment the below cell in order to install dependancies\n",
    "Don't delete the exclamation marks, just the comment sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7107ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install grid2op\n",
    "#!pip install numpy\n",
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e6958",
   "metadata": {},
   "source": [
    "## 0. Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5769242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from grid2op import make\n",
    "from L2RPN_icaps2021_starting_kit.utils import output_dir, problem_dir, score_dir, zip_dir\n",
    "from L2RPN_icaps2021_starting_kit.check_your_submission import main as test_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e6eb0",
   "metadata": {},
   "source": [
    "## 1. Creating a directory for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d380dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../L2RPN_submission_simple/submission'  \n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b41aa2",
   "metadata": {},
   "source": [
    "## 2. Creating model and init.py files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63120a",
   "metadata": {},
   "source": [
    "The cell below is where you can code your agent. Doing nothing to the cell and just running it will generate the DoNothingAgent and only serves as a demonstration on how to format the agent. It would be best to create an agent and then copy and paste the code into the respective functions below. For example, if your __init__() definition was different, you would delete what was in the default block below and paste in your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58082bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../L2RPN_submission_simple/submission/my_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../L2RPN_submission_simple/submission/my_agent.py\n",
    "\n",
    "\n",
    "from grid2op.Agent import BaseAgent\n",
    "\n",
    "class MyAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    The template to be used to create an agent: any controller of the power grid is expected to be a subclass of this\n",
    "    grid2op.Agent.BaseAgent.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        \"\"\"Initialize a new agent.\"\"\"\n",
    "        BaseAgent.__init__(self, action_space=action_space)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"The action that your agent will choose depending on the observation, the reward, and whether the state is terminal\"\"\"\n",
    "        # do nothing for example (with the empty dictionary) :\n",
    "        return self.action_space({})\n",
    "    \n",
    "def make_agent(env, this_directory_path):\n",
    "    my_agent = MyAgent(env.action_space)\n",
    "    return my_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0eba7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../L2RPN_submission_simple/submission/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../L2RPN_submission_simple/submission/__init__.py\n",
    "\n",
    "from .my_agent import make_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b65406",
   "metadata": {},
   "source": [
    "## 3. Testing the submission\n",
    "This will create the zip file, generate submission results and add them to the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca969c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: Basic check and creation of the zip file for the folder ../L2RPN_submission_simple/submission\n",
      "\n",
      "INFO: No custom reward for the assessment of your agent will be used. If you want to use a custom reward when your agent is evaluated, make sure to export  \"reward\", which should be a class inheriting from grid2op.BaseReward in your module (done in __init__.py).\n",
      "INFO: No custom other_rewards for the assessment of your agent will be used. If you want to get information about other rewards when your agent is evaluated, make sure to export  \"other_rewards\" dictionnary in your module (you can do it in the __init__.py file)\n",
      "Your submission appear to be valid. For more test, we encourage you to run the appropriate notebook to do these unit testing.\n",
      "The zip file \"/Users/Conor/Desktop/git_clones/L2RPN_submission_simple/submission_21-08-05.zip\" has been created with your submission in it.\n",
      "\n",
      "INFO: Checking the zip file can be unzipped.\n",
      "\n",
      "\n",
      "INFO: Checking content is valid\n",
      "\n",
      "\n",
      "INFO: metadata found.\n",
      "\n",
      "\n",
      "INFO: This might take a while..\n",
      "It will evaluate your agent on a whole lot of scenarios\n",
      "(24 scenarios, with similar number of timesteps than the private datasets on codalab)\n",
      "\n",
      "\n",
      "INFO: Your agent could be run correctly. \n",
      "You can now check its performance\n",
      "\n",
      "\n",
      "INFO: Check if the results can be read back\n",
      "\n",
      "\n",
      "Your scores are :\n",
      "(remember these score are not at all an indication of what will be used in codalab, as the data it is tested on are really different):\"\n",
      "\n",
      "                 0             1\n",
      "0            score    -56.250000\n",
      "1         duration      0.531093\n",
      "2  total_operation      0.000000\n",
      "3  total_attention   -187.500000\n",
      "\n",
      "------------------------------------\n",
      "         Extra Informations         \n",
      "------------------------------------\n",
      "Don't hesitate to have a look at:\n",
      "        ../L2RPN_submission_simple/submission/last_submission_results/results.html\n",
      "        ../L2RPN_submission_simple/submission/last_submission_results/*.gif\n",
      "To have high level information about your agent.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>score</td>\n",
       "      <td>-56.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duration</td>\n",
       "      <td>0.531093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_operation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_attention</td>\n",
       "      <td>-187.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1\n",
       "0            score    -56.250000\n",
       "1         duration      0.531093\n",
       "2  total_operation      0.000000\n",
       "3  total_attention   -187.500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee9c67",
   "metadata": {},
   "source": [
    "After these cells have been run, you will have both a submission folder and a zip file containing your submission.\n",
    "The .zip file can be submitted directly to codalab, provided that it did not error out on the last cell. Also, please note that the score received on the `test_submission()` function above will not be the score you receive on the codalab leaderboard as it is tested againt a similar but different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de6328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
